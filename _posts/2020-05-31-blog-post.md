---
layout: post
title: "[한글 문서화] Blog Post(1)"
tags: [Documentation]
---

>word_cloud의 [blog post][Blog] (amueller/word-cloud의 개발자가 개발과정을 기록한 문서) 에서  "The integral image is basically a 2d cumulative sum and can be computed as ~" 부분까지 한국어로 번역한 문서입니다.
<hr>

## 1. 개발하게 된 계기

 독일 파이썬 컨퍼런스인 Pycon DE에 참가해서 scikit-learn 작업을 많이 하고 돌아오는 길에, 뭔가 다른걸 해보기로 했습니다. 사실 꽤 이전부터 계획했던 일인데, 그것은 wordle 같은 word cloud를 만드는 것입니다.

 저도 물론 word cloud 같은 것이 좀 구식인 것은 알고있지만 어쨌든 나는 word cloud를 좋아합니다. word cloud를 만들 때 시각화를 좀더 흥미롭게 하기 위해서 topic-model을 활용할 수 있겠다는 생각이 들었습다.

 그래서 저는 쓸만한 word cloud 오픈소스를 찾았는데, 하나도 발견하지 못했습니다. (이건 예전 일이니 지금은 좀 다를 수도 있겠네요.)

 돌아오는 기차 안에서 심심했던 차에 코드를 구상해습니다.

## 2. 초기 개발 과정

![Example1][Ex1]
<br>

 우선 문서를 불러와야 했는데, 저는 미국 헌법 조문을 사용했습니다.

```
 with open("constitution.txt") as f:
        lines f.readlines()                                                                            
    text = "".join(lines) 
```

 그 다음에는 단어에 비중을 두어서 추출해야 했습니다. 예를 들어 그 문서에서 단어가 얼마나 자주 등장하는지를 기준으로 하고자 했습니다. 저는 scikit-learn의  CountVectorizer(역주 : 단어들의 출현빈도로 문서들을 벡터화하는 클래스. 또한 이 과정에서 모두 소문자로 변환한다) 를 사용했습니다. 
 
 저는 가장 많이 등장하는 200개의 단어를의 출현빈도를 얻었고, 가장 많이 등장하는 출현빈도를 통해 정규화했습니다.

```
cv = CountVectorizer(min_df=0, charset_error="ignore",                                               
                         stop_words="english", max_features=200)
counts = cv.fit_transform([text]).toarray().ravel()                                                  
words = np.array(cv.get_feature_names()) 
# normalize                                                                                                                                             
counts = counts / float(counts.max())

```
 이제 본격적인 작업이 시작됐습니다. 가장 기본 아이디어는 캔버스 위의 공간에 무작위로 공간을 추출하는 것입니다. 이때 단어들을 중요도(출현 빈도수)를 기준으로 크기를 조정할 것입니다. 중요한 것은 단어들이 겹치지 않아야 하겠죠.
<br>
 단어들을 이미지로 나타내기 위해서는 Python image library(PIL)만한게 없어보였는데 굉장히 불편하더군요. docstring(역주 : 사용자들을 위한 주석)이 전혀 없었습니다. 
<br>
 아무튼, 아래와 같은 코드를 활용해 이미지를 만들 수 있긴합니다.

```
img_grey = Image.new("L", (width, height))
draw = ImageDraw.Draw(img_grey)

```
그리고나면 이제 아래와 같은 방법으로 이미지 내에 텍스트를  출력할 수 있습니다.

```
font = ImageFont.truetype(font_path, font_size)
draw.setfont(font)
draw.text((y, x), "Text that will appear in white", fill="white")

```
여기서 font_path 는 당신의 시스템에 있는 font의 절대적 경로입니다. 지금은 다른 방법을 알게되긴 했습니다(엄청 어려운 방법은 아닙니다).
<br>
이제 우리는 임의의 위치에 단어를 출력하고, 이때 다른 단어들과 겹치지 않는지 확인해 봐야합니다. ImageDraw 클래스에 사용하기 좋은 함수가 있습니다. 바로 textsize인데요. textsize함수는 단어의 크기가 얼마나 되는지 알려줍니다. 우리는 이를 활용해서 겹치는 부분이 있는지 확인해 볼 수 있겠죠.
<br>
하지만 불행하게도, 이미지 내부에서 임의의 공간을 추출한다는 것은 매우 비효율적임을 알게됐습니다. 한 이미지 내부의 공간이 이미 다른 단어들로 채워진 상황이라면, 
빈 공간을 찾기까지  꽤 많이 시도해봐야 하기 때문이죠.
<br>
그래서 저의 다음 아이디어는 일단 이미지 내부에서 사용할 수 있는  빈 공간을 모두 찾은 뒤, 그들 중에서 임의의 공간을 추출하는 것이었습니다. 이미지 내부에서 빈 공간을 찾기 위한 가장 쉬운 방법은 현재이미지 영역을 원하는 단어의 크기(ImageDraw.textsize(next_word)만큼의 박스(픽셀의 행렬)로 convolution계산하는 것이었습니다.  이 계산의 결과값이 0이 되는 공간들이 바로 단어를 출력할 수 있는 공간들이기 때문입니다. 이 계산을 위해  scipy.ndimage.uniform_filter를 사용했고, 잘 작동했습니다.
<br>
그런데 우리가 원하는 크기의 단어를 출력할 공간이 더이상 없다면 어떻게 해야할까요? 그럴 경우에 글자크기를 좀더 작게 줄인 후, 똑같은 계산을 다시 해봐야합니다. 
<br>
이렇게 해봤는데, 코드는 그렇게 빠르지 않았습니다. 또한 꽤 낭비하는 것처럼 보였죠. 그래서 저는 다른 방법을 쓰고 싶었습니다. 바로 [integral images post][integral images](적분 이미지)입니다. 적분이미지는 이미지 내에서 임의의 직사각형 영역의 합을 추출할 수 있는 2차원 구조를 계산하는 방식입니다.


[integral images wiki]:https://en.wikipedia.org/wiki/Summed-area_table
[Ex1]: https://4.bp.blogspot.com/-cjfJLOPGjIg/UJmCmBa65tI/AAAAAAAAAJk/TF2qRh964GI/s1600/constitution_.pngi
