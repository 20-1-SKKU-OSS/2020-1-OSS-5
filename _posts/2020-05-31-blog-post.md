---
layout: post
title: "[한글 문서화] Blog Post(1)"
tags: [Documentation]
---

>word_cloud의 [blog post][Blog] (amueller/word-cloud의 개발자가 개발과정을 기록한 문서) 에서  "The integral image is basically a 2d cumulative sum and can be computed as ~" 부분까지 한국어로 번역한 문서입니다.
<hr>

## 1. 개발하게 된 계기

 독일 파이썬 컨퍼런스인 Pycon DE에 참가해서 scikit-learn 작업을 많이 하고 돌아오는 길에, 뭔가 다른걸 해보기로 했습니다. 사실 꽤 이전부터 계획했던 일인데, 그것은 wordle 같은 word cloud를 만드는 것입니다.

 저도 물론 word cloud 같은 것이 좀 구식인 것은 알고있지만 어쨌든 나는 word cloud를 좋아합니다. word cloud를 만들 때 시각화를 좀더 흥미롭게 하기 위해서 topic-model을 활용할 수 있겠다는 생각이 들었습다.

 그래서 저는 쓸만한 word cloud 오픈소스를 찾았는데, 하나도 발견하지 못했습니다. (이건 예전 일이니 지금은 좀 다를 수도 있겠네요.)

 돌아오는 기차 안에서 심심했던 차에 코드를 구상해봤다.

## 2. 초기 개발 과정 및 개발 과정 중 어려움

 우선 문서를 불러와야 했는데, 저는 미국 헌법 조문을 사용했다.

```
 with open("constitution.txt") as f:
        lines f.readlines()                                                                            
    text = "".join(lines) 
```

 그 다음에는 단어에 비중을 두어서 추출해야 했습니다. 예를 들어 그 문서에서 단어가 얼마나 자주 등장하는지를 기준으로 하고자 했습니다. 저는 scikit-learn의  CountVectorizer(역주 : 단어들의 출현빈도로 문서들을 벡터화하는 클래스. 또한 이 과정에서 모두 소문자로 변환한다) 를 사용했습니다. 
 
 저는 가장 많이 등장하는 200개의 단어를의 출현빈도를 얻었고, 가장 많이 등장하는 출현빈도를 통해 정규화했습니다.

```
cv = CountVectorizer(min_df=0, charset_error="ignore",                                               
                         stop_words="english", max_features=200)
counts = cv.fit_transform([text]).toarray().ravel()                                                  
words = np.array(cv.get_feature_names()) 
# normalize                                                                                                                                             
counts = counts / float(counts.max())

```


